{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is taken from datacamp website, where the basics are explained in great detail. What I have added here is further explanation of the code, and then some advanced models to make predictions more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing our dataset\n",
    "<p><em>These recommendations are so on point! How does this playlist know me so well?</em></p>\n",
    "<p><img src=\"https://s3.amazonaws.com/assets.datacamp.com/production/project_449/img/iphone_music.jpg\" alt=\"Project Image Record\" width=\"600px\"></p>\n",
    "<p>Over the past few years, streaming services with huge catalogs have become the primary means through which most people listen to their favorite music. But at the same time, the sheer amount of music on offer can mean users might be a bit overwhelmed when trying to look for newer music that suits their tastes.</p>\n",
    "<p>For this reason, streaming services have looked into means of categorizing music to allow for personalized recommendations. One method involves direct analysis of the raw audio information in a given song, scoring the raw data on a variety of metrics. Today, we'll be examining data compiled by a research group known as The Echo Nest. Our goal is to look through this dataset and classify songs as being either 'Hip-Hop' or 'Rock' - all without listening to a single one ourselves. In doing so, we will learn how to clean our data, do some exploratory data visualization, and use feature reduction towards the goal of feeding our data through some simple machine learning algorithms, such as decision trees and logistic regression.</p>\n",
    "<p>To begin with, let's load the metadata about our tracks alongside the track metrics compiled by The Echo Nest. A song is about more than its title, artist, and number of listens. We have another dataset that has musical features of each track such as <code>danceability</code> and <code>acousticness</code> on a scale from -1 to 1. These exist in two different files, which are in different formats - CSV and JSON. While CSV is a popular file format for denoting tabular data, JSON is another common file format in which databases often return the results of a given query.</p>\n",
    "<p>Let's start by creating two pandas <code>DataFrames</code> out of these files that we can merge so we have features and labels (often also referred to as <code>X</code> and <code>y</code>) for the classification later on.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4802 entries, 0 to 4801\n",
      "Data columns (total 10 columns):\n",
      "track_id            4802 non-null int64\n",
      "acousticness        4802 non-null float64\n",
      "danceability        4802 non-null float64\n",
      "energy              4802 non-null float64\n",
      "instrumentalness    4802 non-null float64\n",
      "liveness            4802 non-null float64\n",
      "speechiness         4802 non-null float64\n",
      "tempo               4802 non-null float64\n",
      "valence             4802 non-null float64\n",
      "genre_top           4802 non-null object\n",
      "dtypes: float64(8), int64(1), object(1)\n",
      "memory usage: 412.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4802.000000</td>\n",
       "      <td>4.802000e+03</td>\n",
       "      <td>4802.000000</td>\n",
       "      <td>4802.000000</td>\n",
       "      <td>4802.000000</td>\n",
       "      <td>4802.000000</td>\n",
       "      <td>4802.000000</td>\n",
       "      <td>4802.000000</td>\n",
       "      <td>4802.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>30164.871720</td>\n",
       "      <td>4.870600e-01</td>\n",
       "      <td>0.436556</td>\n",
       "      <td>0.625126</td>\n",
       "      <td>0.604096</td>\n",
       "      <td>0.187997</td>\n",
       "      <td>0.104877</td>\n",
       "      <td>126.687944</td>\n",
       "      <td>0.453413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>28592.013796</td>\n",
       "      <td>3.681396e-01</td>\n",
       "      <td>0.183502</td>\n",
       "      <td>0.244051</td>\n",
       "      <td>0.376487</td>\n",
       "      <td>0.150562</td>\n",
       "      <td>0.145934</td>\n",
       "      <td>34.002473</td>\n",
       "      <td>0.266632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.491000e-07</td>\n",
       "      <td>0.051307</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025297</td>\n",
       "      <td>0.023234</td>\n",
       "      <td>29.093000</td>\n",
       "      <td>0.014392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7494.250000</td>\n",
       "      <td>8.351236e-02</td>\n",
       "      <td>0.296047</td>\n",
       "      <td>0.450757</td>\n",
       "      <td>0.164972</td>\n",
       "      <td>0.104052</td>\n",
       "      <td>0.036897</td>\n",
       "      <td>98.000750</td>\n",
       "      <td>0.224617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>20723.500000</td>\n",
       "      <td>5.156888e-01</td>\n",
       "      <td>0.419447</td>\n",
       "      <td>0.648374</td>\n",
       "      <td>0.808752</td>\n",
       "      <td>0.123080</td>\n",
       "      <td>0.049594</td>\n",
       "      <td>124.625500</td>\n",
       "      <td>0.446240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>44240.750000</td>\n",
       "      <td>8.555765e-01</td>\n",
       "      <td>0.565339</td>\n",
       "      <td>0.837016</td>\n",
       "      <td>0.915472</td>\n",
       "      <td>0.215151</td>\n",
       "      <td>0.088290</td>\n",
       "      <td>151.450000</td>\n",
       "      <td>0.666914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>124722.000000</td>\n",
       "      <td>9.957965e-01</td>\n",
       "      <td>0.961871</td>\n",
       "      <td>0.999768</td>\n",
       "      <td>0.993134</td>\n",
       "      <td>0.971392</td>\n",
       "      <td>0.966177</td>\n",
       "      <td>250.059000</td>\n",
       "      <td>0.983649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            track_id  acousticness  danceability       energy  \\\n",
       "count    4802.000000  4.802000e+03   4802.000000  4802.000000   \n",
       "mean    30164.871720  4.870600e-01      0.436556     0.625126   \n",
       "std     28592.013796  3.681396e-01      0.183502     0.244051   \n",
       "min         2.000000  9.491000e-07      0.051307     0.000279   \n",
       "25%      7494.250000  8.351236e-02      0.296047     0.450757   \n",
       "50%     20723.500000  5.156888e-01      0.419447     0.648374   \n",
       "75%     44240.750000  8.555765e-01      0.565339     0.837016   \n",
       "max    124722.000000  9.957965e-01      0.961871     0.999768   \n",
       "\n",
       "       instrumentalness     liveness  speechiness        tempo      valence  \n",
       "count       4802.000000  4802.000000  4802.000000  4802.000000  4802.000000  \n",
       "mean           0.604096     0.187997     0.104877   126.687944     0.453413  \n",
       "std            0.376487     0.150562     0.145934    34.002473     0.266632  \n",
       "min            0.000000     0.025297     0.023234    29.093000     0.014392  \n",
       "25%            0.164972     0.104052     0.036897    98.000750     0.224617  \n",
       "50%            0.808752     0.123080     0.049594   124.625500     0.446240  \n",
       "75%            0.915472     0.215151     0.088290   151.450000     0.666914  \n",
       "max            0.993134     0.971392     0.966177   250.059000     0.983649  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read in track metadata with genre labels\n",
    "tracks = pd.read_csv(\"fma-rock-vs-hiphop.csv\")\n",
    "\n",
    "# Read in track metrics with the features\n",
    "echonest_metrics = pd.read_json(\"echonest-metrics.json\", precise_float = True)\n",
    "\n",
    "# Merge the relevant columns of tracks and echonest_metrics\n",
    "echo_tracks = pd.merge(echonest_metrics, tracks[[\"track_id\", \"genre_top\"]], on=\"track_id\")\n",
    "\n",
    "#Here, we are merging two datasets- echonest_metrics and tracks, but we are retaining only two columns from tracks dataset - track_id and genre_top.\n",
    "#We are merging the dataset based on track_id\n",
    "\n",
    "# Inspect the resultant dataframe\n",
    "echo_tracks.info()\n",
    "echo_tracks.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "precise_float is a parameter set to enable usage of higher precision (strtod) function when decoding string to double values. Default (False) is to use fast but less precise builtin functionality. For better understanding, here is a great article demonstrating the strtod function. (https://www.geeksforgeeks.org/strtod-function-in-c-c/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pairwise relationships between continuous variables\n",
    "<p>We typically want to avoid using variables that have strong correlations with each other -- hence avoiding feature redundancy -- for a few reasons:</p>\n",
    "<ul>\n",
    "<li>To keep the model simple and improve interpretability (with many features, we run the risk of overfitting).</li>\n",
    "<li>When our datasets are very large, using fewer features can drastically speed up our computation time.</li>\n",
    "</ul>\n",
    "<p>To get a sense of whether there are any strongly correlated features in our data, we will use built-in functions in the <code>pandas</code> package.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_f049210c_ca32_11e9_a789_58a02312a5b5row0_col0 {\n",
       "            background-color:  #023858;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row0_col1 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row0_col2 {\n",
       "            background-color:  #d2d2e7;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row0_col3 {\n",
       "            background-color:  #b5c4df;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row0_col4 {\n",
       "            background-color:  #f5eef6;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row0_col5 {\n",
       "            background-color:  #e9e5f1;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row0_col6 {\n",
       "            background-color:  #d1d2e6;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row0_col7 {\n",
       "            background-color:  #e1dfed;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row0_col8 {\n",
       "            background-color:  #dedcec;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row1_col0 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row1_col1 {\n",
       "            background-color:  #023858;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row1_col2 {\n",
       "            background-color:  #e0dded;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row1_col3 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row1_col4 {\n",
       "            background-color:  #97b7d7;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row1_col5 {\n",
       "            background-color:  #f3edf5;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row1_col6 {\n",
       "            background-color:  #b8c6e0;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row1_col7 {\n",
       "            background-color:  #e1dfed;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row1_col8 {\n",
       "            background-color:  #e2dfee;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row2_col0 {\n",
       "            background-color:  #bdc8e1;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row2_col1 {\n",
       "            background-color:  #d0d1e6;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row2_col2 {\n",
       "            background-color:  #023858;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row2_col3 {\n",
       "            background-color:  #fbf3f9;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row2_col4 {\n",
       "            background-color:  #f3edf5;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row2_col5 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row2_col6 {\n",
       "            background-color:  #80aed2;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row2_col7 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row2_col8 {\n",
       "            background-color:  #529bc7;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row3_col0 {\n",
       "            background-color:  #a7bddb;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row3_col1 {\n",
       "            background-color:  #f5eff6;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row3_col2 {\n",
       "            background-color:  #fef6fa;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row3_col3 {\n",
       "            background-color:  #023858;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row3_col4 {\n",
       "            background-color:  #c4cbe3;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row3_col5 {\n",
       "            background-color:  #dcdaeb;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row3_col6 {\n",
       "            background-color:  #dedcec;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row3_col7 {\n",
       "            background-color:  #adc1dd;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row3_col8 {\n",
       "            background-color:  #d9d8ea;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row4_col0 {\n",
       "            background-color:  #f4eef6;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row4_col1 {\n",
       "            background-color:  #97b7d7;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row4_col2 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row4_col3 {\n",
       "            background-color:  #d2d3e7;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row4_col4 {\n",
       "            background-color:  #023858;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row4_col5 {\n",
       "            background-color:  #fdf5fa;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row4_col6 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row4_col7 {\n",
       "            background-color:  #d9d8ea;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row4_col8 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row5_col0 {\n",
       "            background-color:  #bdc8e1;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row5_col1 {\n",
       "            background-color:  #ced0e6;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row5_col2 {\n",
       "            background-color:  #ede8f3;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row5_col3 {\n",
       "            background-color:  #bdc8e1;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row5_col4 {\n",
       "            background-color:  #dbdaeb;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row5_col5 {\n",
       "            background-color:  #023858;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row5_col6 {\n",
       "            background-color:  #c0c9e2;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row5_col7 {\n",
       "            background-color:  #dcdaeb;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row5_col8 {\n",
       "            background-color:  #e8e4f0;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row6_col0 {\n",
       "            background-color:  #d0d1e6;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row6_col1 {\n",
       "            background-color:  #b8c6e0;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row6_col2 {\n",
       "            background-color:  #93b5d6;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row6_col3 {\n",
       "            background-color:  #eae6f1;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row6_col4 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row6_col5 {\n",
       "            background-color:  #eae6f1;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row6_col6 {\n",
       "            background-color:  #023858;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row6_col7 {\n",
       "            background-color:  #dbdaeb;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row6_col8 {\n",
       "            background-color:  #bfc9e1;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row7_col0 {\n",
       "            background-color:  #d0d1e6;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row7_col1 {\n",
       "            background-color:  #d0d1e6;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row7_col2 {\n",
       "            background-color:  #fef6fa;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row7_col3 {\n",
       "            background-color:  #a7bddb;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row7_col4 {\n",
       "            background-color:  #c5cce3;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row7_col5 {\n",
       "            background-color:  #f0eaf4;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row7_col6 {\n",
       "            background-color:  #c8cde4;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row7_col7 {\n",
       "            background-color:  #023858;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row7_col8 {\n",
       "            background-color:  #d6d6e9;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row8_col0 {\n",
       "            background-color:  #c6cce3;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row8_col1 {\n",
       "            background-color:  #cdd0e5;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row8_col2 {\n",
       "            background-color:  #4c99c5;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row8_col3 {\n",
       "            background-color:  #d1d2e6;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row8_col4 {\n",
       "            background-color:  #efe9f3;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row8_col5 {\n",
       "            background-color:  #f7f0f7;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row8_col6 {\n",
       "            background-color:  #a5bddb;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row8_col7 {\n",
       "            background-color:  #d3d4e7;\n",
       "            color:  #000000;\n",
       "        }    #T_f049210c_ca32_11e9_a789_58a02312a5b5row8_col8 {\n",
       "            background-color:  #023858;\n",
       "            color:  #f1f1f1;\n",
       "        }</style><table id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >track_id</th>        <th class=\"col_heading level0 col1\" >acousticness</th>        <th class=\"col_heading level0 col2\" >danceability</th>        <th class=\"col_heading level0 col3\" >energy</th>        <th class=\"col_heading level0 col4\" >instrumentalness</th>        <th class=\"col_heading level0 col5\" >liveness</th>        <th class=\"col_heading level0 col6\" >speechiness</th>        <th class=\"col_heading level0 col7\" >tempo</th>        <th class=\"col_heading level0 col8\" >valence</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5level0_row0\" class=\"row_heading level0 row0\" >track_id</th>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row0_col1\" class=\"data row0 col1\" >-0.372282</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row0_col2\" class=\"data row0 col2\" >0.0494541</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row0_col3\" class=\"data row0 col3\" >0.140703</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row0_col4\" class=\"data row0 col4\" >-0.275623</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row0_col5\" class=\"data row0 col5\" >0.0482307</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row0_col6\" class=\"data row0 col6\" >-0.0269951</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row0_col7\" class=\"data row0 col7\" >-0.0253918</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row0_col8\" class=\"data row0 col8\" >0.0100698</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5level0_row1\" class=\"row_heading level0 row1\" >acousticness</th>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row1_col0\" class=\"data row1 col0\" >-0.372282</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row1_col1\" class=\"data row1 col1\" >1</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row1_col2\" class=\"data row1 col2\" >-0.0289537</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row1_col3\" class=\"data row1 col3\" >-0.281619</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row1_col4\" class=\"data row1 col4\" >0.19478</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row1_col5\" class=\"data row1 col5\" >-0.0199914</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row1_col6\" class=\"data row1 col6\" >0.072204</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row1_col7\" class=\"data row1 col7\" >-0.0263097</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row1_col8\" class=\"data row1 col8\" >-0.0138406</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5level0_row2\" class=\"row_heading level0 row2\" >danceability</th>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row2_col0\" class=\"data row2 col0\" >0.0494541</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row2_col1\" class=\"data row2 col1\" >-0.0289537</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row2_col2\" class=\"data row2 col2\" >1</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row2_col3\" class=\"data row2 col3\" >-0.242032</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row2_col4\" class=\"data row2 col4\" >-0.255217</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row2_col5\" class=\"data row2 col5\" >-0.106584</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row2_col6\" class=\"data row2 col6\" >0.276206</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row2_col7\" class=\"data row2 col7\" >-0.242089</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row2_col8\" class=\"data row2 col8\" >0.473165</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5level0_row3\" class=\"row_heading level0 row3\" >energy</th>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row3_col0\" class=\"data row3 col0\" >0.140703</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row3_col1\" class=\"data row3 col1\" >-0.281619</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row3_col2\" class=\"data row3 col2\" >-0.242032</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row3_col3\" class=\"data row3 col3\" >1</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row3_col4\" class=\"data row3 col4\" >0.0282377</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row3_col5\" class=\"data row3 col5\" >0.113331</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row3_col6\" class=\"data row3 col6\" >-0.109983</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row3_col7\" class=\"data row3 col7\" >0.195227</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row3_col8\" class=\"data row3 col8\" >0.0386027</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5level0_row4\" class=\"row_heading level0 row4\" >instrumentalness</th>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row4_col0\" class=\"data row4 col0\" >-0.275623</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row4_col1\" class=\"data row4 col1\" >0.19478</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row4_col2\" class=\"data row4 col2\" >-0.255217</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row4_col3\" class=\"data row4 col3\" >0.0282377</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row4_col4\" class=\"data row4 col4\" >1</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row4_col5\" class=\"data row4 col5\" >-0.0910218</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row4_col6\" class=\"data row4 col6\" >-0.366762</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row4_col7\" class=\"data row4 col7\" >0.022215</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row4_col8\" class=\"data row4 col8\" >-0.219967</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5level0_row5\" class=\"row_heading level0 row5\" >liveness</th>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row5_col0\" class=\"data row5 col0\" >0.0482307</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row5_col1\" class=\"data row5 col1\" >-0.0199914</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row5_col2\" class=\"data row5 col2\" >-0.106584</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row5_col3\" class=\"data row5 col3\" >0.113331</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row5_col4\" class=\"data row5 col4\" >-0.0910218</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row5_col5\" class=\"data row5 col5\" >1</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row5_col6\" class=\"data row5 col6\" >0.0411725</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row5_col7\" class=\"data row5 col7\" >0.00273169</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row5_col8\" class=\"data row5 col8\" >-0.0450931</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5level0_row6\" class=\"row_heading level0 row6\" >speechiness</th>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row6_col0\" class=\"data row6 col0\" >-0.0269951</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row6_col1\" class=\"data row6 col1\" >0.072204</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row6_col2\" class=\"data row6 col2\" >0.276206</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row6_col3\" class=\"data row6 col3\" >-0.109983</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row6_col4\" class=\"data row6 col4\" >-0.366762</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row6_col5\" class=\"data row6 col5\" >0.0411725</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row6_col6\" class=\"data row6 col6\" >1</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row6_col7\" class=\"data row6 col7\" >0.00824055</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row6_col8\" class=\"data row6 col8\" >0.149894</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5level0_row7\" class=\"row_heading level0 row7\" >tempo</th>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row7_col0\" class=\"data row7 col0\" >-0.0253918</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row7_col1\" class=\"data row7 col1\" >-0.0263097</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row7_col2\" class=\"data row7 col2\" >-0.242089</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row7_col3\" class=\"data row7 col3\" >0.195227</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row7_col4\" class=\"data row7 col4\" >0.022215</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row7_col5\" class=\"data row7 col5\" >0.00273169</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row7_col6\" class=\"data row7 col6\" >0.00824055</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row7_col7\" class=\"data row7 col7\" >1</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row7_col8\" class=\"data row7 col8\" >0.0522212</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5level0_row8\" class=\"row_heading level0 row8\" >valence</th>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row8_col0\" class=\"data row8 col0\" >0.0100698</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row8_col1\" class=\"data row8 col1\" >-0.0138406</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row8_col2\" class=\"data row8 col2\" >0.473165</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row8_col3\" class=\"data row8 col3\" >0.0386027</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row8_col4\" class=\"data row8 col4\" >-0.219967</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row8_col5\" class=\"data row8 col5\" >-0.0450931</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row8_col6\" class=\"data row8 col6\" >0.149894</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row8_col7\" class=\"data row8 col7\" >0.0522212</td>\n",
       "                        <td id=\"T_f049210c_ca32_11e9_a789_58a02312a5b5row8_col8\" class=\"data row8 col8\" >1</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1373af47e80>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a correlation matrix\n",
    "corr_metrics = echo_tracks.corr()\n",
    "corr_metrics.style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put in plain terms, correlation is a measure of how strongly one variable depends on another.\n",
    "\n",
    "Correlation can be an important tool for feature engineering in building machine learning models. Predictors which are uncorrelated with the objective variable are probably good candidates to trim from the model (shoe size is not a useful predictor for salary). In addition, if two predictors are strongly correlated to each other, then we only need to use one of them (in predicting salary, there is no need to use both age in years, and age in months). Taking these steps means that the resulting model will be simpler, and simpler models are easier to interpret.\n",
    "\n",
    "Refer to this great article (https://blog.bigml.com/2015/09/21/looking-for-connections-in-your-data-correlation-coefficients/) for further explanation on correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalizing the feature data\n",
    "<p>As mentioned earlier, it can be particularly useful to simplify our models and use as few features as necessary to achieve the best result. Since we didn't find any particular strong correlations between our features, we can instead use a common approach to reduce the number of features called <strong>principal component analysis (PCA)</strong>. </p>\n",
    "\n",
    "Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated variables called principal components.\n",
    "\n",
    "Here is a very nice video explaining PCA in detail (https://www.youtube.com/watch?v=FgakZw6K1QQ&vl=en).\n",
    "\n",
    "<p>It is possible that the variance between genres can be explained by just a few features in the dataset. PCA rotates the data along the axis of highest variance, thus allowing us to determine the relative contribution of each feature of our data towards the variance between classes. </p>\n",
    "<p>However, since PCA uses the absolute variance of a feature to rotate the data, a feature with a broader range of values will overpower and bias the algorithm relative to the other features. To avoid this, we must first normalize our data. There are a few methods to do this, but a common way is through <em>standardization</em>, such that all features have a mean = 0 and standard deviation = 1 (the resultant is a z-score).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our features \n",
    "features = echo_tracks.drop([\"genre_top\",\"track_id\"], axis=1)\n",
    "\n",
    "# Define our labels\n",
    "labels = echo_tracks[\"genre_top\"]\n",
    "\n",
    "# Import the StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the features and set the values to a new variable\n",
    "scaler = StandardScaler()\n",
    "scaled_train_features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Principal Component Analysis on our scaled data\n",
    "<p>Now that we have preprocessed our data, we are ready to use PCA to determine by how much we can reduce the dimensionality of our data. We can use <strong>scree-plots</strong> and <strong>cumulative explained ratio plots</strong> to find the number of components to use in further analyses.</p>\n",
    "<p>Scree-plots display the number of components against the variance explained by each component, sorted in descending order of variance. Scree-plots help us get a better sense of which components explain a sufficient amount of variance in our data. When using scree plots, an 'elbow' (a steep drop from one data point to the next) in the plot is typically used to decide on an appropriate cutoff.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24297674 0.18044316 0.13650309 0.12994089 0.11056248 0.08302245\n",
      " 0.06923783 0.04731336]\n",
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Principal Component #')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is just to make plots appear in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Import our plotting module, and PCA class\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Get our explained variance ratios from PCA using all features\n",
    "pca = PCA()\n",
    "pca.fit(scaled_train_features)\n",
    "exp_variance = pca.explained_variance_ratio_\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.n_components_)\n",
    "\n",
    "# plot the explained variance using a barplot\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(range(8), exp_variance)\n",
    "ax.set_xlabel('Principal Component #')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Further visualization of PCA\n",
    "<p>Unfortunately, there does not appear to be a clear elbow in this scree plot, which means it is not straightforward to find the number of intrinsic dimensions using this method. </p>\n",
    "<p>But all is not lost! Instead, we can also look at the <strong>cumulative explained variance plot</strong> to determine how many features are required to explain, say, about 90% of the variance (cutoffs are somewhat arbitrary here, and usually decided upon by 'rules of thumb'). Once we determine the appropriate number of components, we can perform PCA with that many components, ideally reducing the dimensionality of our data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Calculate the cumulative explained variance\n",
    "cum_exp_variance = np.cumsum(exp_variance)\n",
    "\n",
    "# Plot the cumulative explained variance and draw a dashed line at 0.90.\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(8), cum_exp_variance)\n",
    "ax.axhline(y=0.9, linestyle='--')\n",
    "n_components = 6\n",
    "\n",
    "# Perform PCA with the chosen number of components and project data onto components\n",
    "pca = PCA(n_components, random_state=10)\n",
    "pca.fit(scaled_train_features)\n",
    "pca_projection = pca.transform(scaled_train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Balance our data for greater performance\n",
    "<p>Both our models do similarly well, boasting an average precision of 87% each. However, looking at our classification report, we can see that rock songs are fairly well classified, but hip-hop songs are disproportionately misclassified as rock songs. </p>\n",
    "<p>Why might this be the case? Well, just by looking at the number of data points we have for each class, we see that we have far more data points for the rock classification than for hip-hop, potentially skewing our model's ability to distinguish between classes. This also tells us that most of our model's accuracy is driven by its ability to classify just rock songs, which is less than ideal.</p>\n",
    "<p>To account for this, we can weight the value of a correct classification in each class inversely to the occurrence of data points for each class. Since a correct classification for \"Rock\" is not more important than a correct classification for \"Hip-Hop\" (and vice versa), we only need to account for differences in <em>sample size</em> of our data points when weighting our classes here, and not relative importance of each class. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset only the hip-hop tracks, and then only the rock tracks\n",
    "hop_only = echo_tracks.loc[echo_tracks[\"genre_top\"] == \"Hip-Hop\"]\n",
    "rock_only = echo_tracks.loc[echo_tracks[\"genre_top\"] == \"Rock\"]\n",
    "\n",
    "# sample the rocks songs to be the same number as there are hip-hop songs\n",
    "rock_only = rock_only.sample(len(hop_only), random_state=10)\n",
    "\n",
    "# concatenate the dataframes rock_only and hop_only\n",
    "rock_hop_bal = pd.concat([rock_only, hop_only])\n",
    "\n",
    "# The features, labels, and pca projection are created for the balanced dataframe\n",
    "features = rock_hop_bal.drop(['genre_top', 'track_id'], axis=1) \n",
    "labels = rock_hop_bal['genre_top']\n",
    "pca_projection = pca.fit_transform(scaler.fit_transform(features))\n",
    "\n",
    "# Import train_test_split function and Decision tree classifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the train and test set with the pca_projection from the balanced data\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(pca_projection, labels, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Understanding the Evaluation Metrics\n",
    "\n",
    "Before we move onto the algorithms, it is crucial to understand the metric we are going to use to evaluate our predictive models.\n",
    "\n",
    "Accuracy - Accuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations. One may think that, if we have high accuracy then our model is best. Yes, accuracy is a great measure but only when you have symmetric datasets where values of false positive and false negatives are almost same. Therefore, you have to look at other parameters to evaluate the performance of your model. For our model, we have got 0.803 which means our model is approx. 80% accurate.\n",
    "\n",
    "Accuracy = TP+TN/TP+FP+FN+TN\n",
    "\n",
    "Precision - Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. The question that this metric answer is of all passengers that labeled as survived, how many actually survived? High precision relates to the low false positive rate. We have got 0.788 precision which is pretty good.\n",
    "\n",
    "Precision = TP/TP+FP\n",
    "\n",
    "Recall (Sensitivity) - Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes. The question recall answers is: Of all the passengers that truly survived, how many did we label? We have got recall of 0.631 which is good for this model as it’s above 0.5.\n",
    "\n",
    "Recall = TP/TP+FN\n",
    "\n",
    "F1 score - F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it’s better to look at both Precision and Recall. In our case, F1 score is 0.701.\n",
    "\n",
    "F1 Score = 2*(Recall * Precision) / (Recall + Precision)\n",
    "\n",
    "As you may have got an idea, the evaluation metric that we are going to use is Accuracy. The evaluation metric depends on the type of data that you are analyzing. If we were handling a sensitive problem (like cancer test, loan application) then false positives matter a lot. But, we are only dealing with songs, and no one label is more crucia or sensitive than the other. Hence, accuracy is what we will be more concerned with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Applying the Algorithms\n",
    "\n",
    "This is the part where we start creating different predictive models and evaluate them for their accuracy. I'll also breifly mention the mathematics behind the model and provide reference for further information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajink\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 28 candidates, totalling 84 fits\n",
      "Accuracy of the KNN model is:  0.832967032967033\n",
      "KNN model best parameters are:  {'metric': 'manhattan', 'n_neighbors': 5, 'weights': 'distance'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  84 out of  84 | elapsed:    2.3s finished\n"
     ]
    }
   ],
   "source": [
    "# Create the classification report for all the models we shall use\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "#Importing a package to fine-tune and optimize our algorithms\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "knn_grid_params = {\n",
    "    'n_neighbors': [1,2,3,4,5,6,7],\n",
    "    'weights': ['uniform','distance'],\n",
    "    'metric': ['euclidean','manhattan']\n",
    "}\n",
    "knn_gs = GridSearchCV(KNeighborsClassifier(),knn_grid_params,verbose=1, n_jobs = -1)\n",
    "knn_gs_results = knn_gs.fit(train_features, train_labels)\n",
    "print(\"Accuracy of the KNN model is: \", knn_gs_results.best_score_)\n",
    "print(\"KNN model best parameters are: \", knn_gs_results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions).\n",
    "\n",
    "Reference: https://medium.com/@erikgreenj/k-neighbors-classifier-with-gridsearchcv-basics-3c445ddeb657"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajink\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 75 candidates, totalling 225 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 225 out of 225 | elapsed:    2.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Logistic Regression model is:  0.8161172161172161\n",
      "Logistic Regression model best parameters are:  {'C': 10, 'l1_ratio': 0, 'penalty': 'l2', 'solver': 'saga'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajink\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1506: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  \"(penalty={})\".format(self.penalty))\n"
     ]
    }
   ],
   "source": [
    "# Train our logistic regression on the balanced data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logit_grid_params = {\n",
    "    'C':[0.001,0.1,1,10,100],\n",
    "    'penalty':['l1','l2','elasticnet'],\n",
    "    'solver':['saga'],\n",
    "    'l1_ratio':[0,0.1,0.5,0.7,1]\n",
    "}\n",
    "logit_gs = GridSearchCV(LogisticRegression(),logit_grid_params,verbose=1, n_jobs = -1)\n",
    "logit_gs_results = logit_gs.fit(train_features, train_labels)\n",
    "print(\"Accuracy of the Logistic Regression model is: \", logit_gs_results.best_score_)\n",
    "print(\"Logistic Regression model best parameters are: \", logit_gs_results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not quite sure about the parameters used in this algorithm. I am researching on it to find exactly what does each parameter specify and how it affects the predictive capabilities of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajink\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    2.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Decision Tree Classifier model is:  0.7743589743589744\n",
      "Decision Tree Classifier model best parameters are:  {'criterion': 'entropy', 'min_samples_leaf': 4, 'min_samples_split': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 216 out of 216 | elapsed:    2.4s finished\n"
     ]
    }
   ],
   "source": [
    "# Train our decision tree on the balanced data\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree_grid_params = {\n",
    "    'criterion': ['gini','entropy'],\n",
    "    'min_samples_split': [2,3,4,5,6,7,8,9,10],\n",
    "    'min_samples_leaf' : [1,2,3,4],\n",
    "}\n",
    "tree_gs = GridSearchCV(DecisionTreeClassifier(),tree_grid_params,verbose=1, n_jobs = -1)\n",
    "tree_gs_results = tree_gs.fit(train_features, train_labels)\n",
    "print(\"Accuracy of the Decision Tree Classifier model is: \", tree_gs_results.best_score_)\n",
    "print(\"Decision Tree Classifier model best parameters are: \", tree_gs_results.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, decision tree classifier gives us a very low accuracy. The best from the models we evaluated so far is the KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajink\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  30 | elapsed:    0.1s remaining:    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Random Forest Classifier model is:  0.8344322344322345\n",
      "Random Forest Classifier model best parameters are:  {'criterion': 'entropy', 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# Train our random forest on the balanced data\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_grid_params = {\n",
    "    'n_estimators': [60,70,80,90,100],\n",
    "    'criterion': ['gini','entropy']\n",
    "}\n",
    "rf_gs = GridSearchCV(RandomForestClassifier(),rf_grid_params,verbose=1, n_jobs = -1)\n",
    "rf_gs_results = rf_gs.fit(train_features, train_labels)\n",
    "print(\"Accuracy of the Random Forest Classifier model is: \", rf_gs_results.best_score_)\n",
    "print(\"Random Forest Classifier model best parameters are: \", rf_gs_results.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 xG boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajink\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   22.5s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   26.1s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   28.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Gradient Boosting Classifier model is:  0.8234432234432234\n",
      "Gradient Boosting Classifier model best parameters are:  {'learning_rate': 0.5, 'loss': 'exponential', 'n_estimators': 100, 'warm_start': 'True'}\n"
     ]
    }
   ],
   "source": [
    "# Train our XG Boost model on the balanced data\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb_grid_params = {\n",
    "    'loss': ['deviance','exponential'],\n",
    "    'learning_rate': [0.01,0.04,0.1,0.2,0.5],\n",
    "    'n_estimators': [60,70,80,90,100],\n",
    "    'warm_start': ['True','False']\n",
    "}\n",
    "gb_gs = GridSearchCV(GradientBoostingClassifier(),gb_grid_params,verbose=1, n_jobs = -1)\n",
    "gb_gs_results = gb_gs.fit(train_features, train_labels)\n",
    "print(\"Accuracy of the Gradient Boosting Classifier model is: \", gb_gs_results.best_score_)\n",
    "print(\"Gradient Boosting Classifier model best parameters are: \", gb_gs_results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajink\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    5.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the SVC model is:  0.8395604395604396\n",
      "SVC model best parameters are:  {'C': 1, 'degree': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:    6.4s finished\n",
      "C:\\Users\\ajink\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Train our SVM model on the balanced data\n",
    "from sklearn.svm import SVC\n",
    "svc_grid_params = {\n",
    "    'C':[0.001,0.1,1,10,100],\n",
    "    'degree':[1,2,3,4,5],\n",
    "    }\n",
    "svc_gs = GridSearchCV(SVC(),svc_grid_params,verbose=1, n_jobs = -1)\n",
    "svc_gs_results = svc_gs.fit(train_features, train_labels)\n",
    "print(\"Accuracy of the SVC model is: \", svc_gs_results.best_score_)\n",
    "print(\"SVC model best parameters are: \", svc_gs_results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7 Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.73463753\n",
      "Iteration 2, loss = 0.72930827\n",
      "Iteration 3, loss = 0.72178216\n",
      "Iteration 4, loss = 0.71368715\n",
      "Iteration 5, loss = 0.70534230\n",
      "Iteration 6, loss = 0.69734020\n",
      "Iteration 7, loss = 0.68940567\n",
      "Iteration 8, loss = 0.68179058\n",
      "Iteration 9, loss = 0.67444407\n",
      "Iteration 10, loss = 0.66727419\n",
      "Iteration 11, loss = 0.66016361\n",
      "Iteration 12, loss = 0.65333306\n",
      "Iteration 13, loss = 0.64666227\n",
      "Iteration 14, loss = 0.63983817\n",
      "Iteration 15, loss = 0.63321061\n",
      "Iteration 16, loss = 0.62653774\n",
      "Iteration 17, loss = 0.62002353\n",
      "Iteration 18, loss = 0.61337436\n",
      "Iteration 19, loss = 0.60685978\n",
      "Iteration 20, loss = 0.60026066\n",
      "Iteration 21, loss = 0.59376147\n",
      "Iteration 22, loss = 0.58727203\n",
      "Iteration 23, loss = 0.58068279\n",
      "Iteration 24, loss = 0.57440092\n",
      "Iteration 25, loss = 0.56793278\n",
      "Iteration 26, loss = 0.56159590\n",
      "Iteration 27, loss = 0.55518675\n",
      "Iteration 28, loss = 0.54893490\n",
      "Iteration 29, loss = 0.54261992\n",
      "Iteration 30, loss = 0.53670881\n",
      "Iteration 31, loss = 0.53062439\n",
      "Iteration 32, loss = 0.52464312\n",
      "Iteration 33, loss = 0.51882736\n",
      "Iteration 34, loss = 0.51317836\n",
      "Iteration 35, loss = 0.50763664\n",
      "Iteration 36, loss = 0.50219957\n",
      "Iteration 37, loss = 0.49714328\n",
      "Iteration 38, loss = 0.49205113\n",
      "Iteration 39, loss = 0.48719225\n",
      "Iteration 40, loss = 0.48249310\n",
      "Iteration 41, loss = 0.47806786\n",
      "Iteration 42, loss = 0.47373825\n",
      "Iteration 43, loss = 0.46972207\n",
      "Iteration 44, loss = 0.46572654\n",
      "Iteration 45, loss = 0.46209637\n",
      "Iteration 46, loss = 0.45859950\n",
      "Iteration 47, loss = 0.45522785\n",
      "Iteration 48, loss = 0.45216921\n",
      "Iteration 49, loss = 0.44914325\n",
      "Iteration 50, loss = 0.44636937\n",
      "Iteration 51, loss = 0.44365991\n",
      "Iteration 52, loss = 0.44126998\n",
      "Iteration 53, loss = 0.43877669\n",
      "Iteration 54, loss = 0.43662781\n",
      "Iteration 55, loss = 0.43451388\n",
      "Iteration 56, loss = 0.43261507\n",
      "Iteration 57, loss = 0.43073508\n",
      "Iteration 58, loss = 0.42903024\n",
      "Iteration 59, loss = 0.42736385\n",
      "Iteration 60, loss = 0.42584447\n",
      "Iteration 61, loss = 0.42439862\n",
      "Iteration 62, loss = 0.42307151\n",
      "Iteration 63, loss = 0.42168351\n",
      "Iteration 64, loss = 0.42058680\n",
      "Iteration 65, loss = 0.41932058\n",
      "Iteration 66, loss = 0.41831384\n",
      "Iteration 67, loss = 0.41725281\n",
      "Iteration 68, loss = 0.41620394\n",
      "Iteration 69, loss = 0.41532302\n",
      "Iteration 70, loss = 0.41437297\n",
      "Iteration 71, loss = 0.41366041\n",
      "Iteration 72, loss = 0.41275358\n",
      "Iteration 73, loss = 0.41201831\n",
      "Iteration 74, loss = 0.41125141\n",
      "Iteration 75, loss = 0.41053909\n",
      "Iteration 76, loss = 0.40986108\n",
      "Iteration 77, loss = 0.40924932\n",
      "Iteration 78, loss = 0.40856734\n",
      "Iteration 79, loss = 0.40805742\n",
      "Iteration 80, loss = 0.40742038\n",
      "Iteration 81, loss = 0.40680651\n",
      "Iteration 82, loss = 0.40628103\n",
      "Iteration 83, loss = 0.40576710\n",
      "Iteration 84, loss = 0.40521707\n",
      "Iteration 85, loss = 0.40472106\n",
      "Iteration 86, loss = 0.40419343\n",
      "Iteration 87, loss = 0.40371957\n",
      "Iteration 88, loss = 0.40330634\n",
      "Iteration 89, loss = 0.40278502\n",
      "Iteration 90, loss = 0.40234114\n",
      "Iteration 91, loss = 0.40188573\n",
      "Iteration 92, loss = 0.40143156\n",
      "Iteration 93, loss = 0.40104259\n",
      "Iteration 94, loss = 0.40057833\n",
      "Iteration 95, loss = 0.40017118\n",
      "Iteration 96, loss = 0.39978608\n",
      "Iteration 97, loss = 0.39934519\n",
      "Iteration 98, loss = 0.39897834\n",
      "Iteration 99, loss = 0.39856793\n",
      "Iteration 100, loss = 0.39819531\n",
      "Iteration 101, loss = 0.39781544\n",
      "Iteration 102, loss = 0.39743921\n",
      "Iteration 103, loss = 0.39706115\n",
      "Iteration 104, loss = 0.39669346\n",
      "Iteration 105, loss = 0.39635119\n",
      "Iteration 106, loss = 0.39596694\n",
      "Iteration 107, loss = 0.39564405\n",
      "Iteration 108, loss = 0.39527062\n",
      "Iteration 109, loss = 0.39494387\n",
      "Iteration 110, loss = 0.39458984\n",
      "Iteration 111, loss = 0.39426497\n",
      "Iteration 112, loss = 0.39390576\n",
      "Iteration 113, loss = 0.39358775\n",
      "Iteration 114, loss = 0.39328209\n",
      "Iteration 115, loss = 0.39294050\n",
      "Iteration 116, loss = 0.39262006\n",
      "Iteration 117, loss = 0.39229116\n",
      "Iteration 118, loss = 0.39198551\n",
      "Iteration 119, loss = 0.39162184\n",
      "Iteration 120, loss = 0.39133417\n",
      "Iteration 121, loss = 0.39102136\n",
      "Iteration 122, loss = 0.39069050\n",
      "Iteration 123, loss = 0.39043113\n",
      "Iteration 124, loss = 0.39011268\n",
      "Iteration 125, loss = 0.38978617\n",
      "Iteration 126, loss = 0.38947134\n",
      "Iteration 127, loss = 0.38916681\n",
      "Iteration 128, loss = 0.38888914\n",
      "Iteration 129, loss = 0.38857622\n",
      "Iteration 130, loss = 0.38827701\n",
      "Iteration 131, loss = 0.38798601\n",
      "Iteration 132, loss = 0.38771044\n",
      "Iteration 133, loss = 0.38744250\n",
      "Iteration 134, loss = 0.38717911\n",
      "Iteration 135, loss = 0.38682813\n",
      "Iteration 136, loss = 0.38654843\n",
      "Iteration 137, loss = 0.38623567\n",
      "Iteration 138, loss = 0.38596470\n",
      "Iteration 139, loss = 0.38567657\n",
      "Iteration 140, loss = 0.38542450\n",
      "Iteration 141, loss = 0.38510303\n",
      "Iteration 142, loss = 0.38484367\n",
      "Iteration 143, loss = 0.38454048\n",
      "Iteration 144, loss = 0.38424196\n",
      "Iteration 145, loss = 0.38393916\n",
      "Iteration 146, loss = 0.38368257\n",
      "Iteration 147, loss = 0.38340140\n",
      "Iteration 148, loss = 0.38311414\n",
      "Iteration 149, loss = 0.38283666\n",
      "Iteration 150, loss = 0.38256495\n",
      "Iteration 151, loss = 0.38227376\n",
      "Iteration 152, loss = 0.38199106\n",
      "Iteration 153, loss = 0.38173036\n",
      "Iteration 154, loss = 0.38147329\n",
      "Iteration 155, loss = 0.38116768\n",
      "Iteration 156, loss = 0.38091316\n",
      "Iteration 157, loss = 0.38062573\n",
      "Iteration 158, loss = 0.38037117\n",
      "Iteration 159, loss = 0.38009359\n",
      "Iteration 160, loss = 0.37987049\n",
      "Iteration 161, loss = 0.37955163\n",
      "Iteration 162, loss = 0.37931500\n",
      "Iteration 163, loss = 0.37901692\n",
      "Iteration 164, loss = 0.37880629\n",
      "Iteration 165, loss = 0.37846139\n",
      "Iteration 166, loss = 0.37824546\n",
      "Iteration 167, loss = 0.37792260\n",
      "Iteration 168, loss = 0.37765815\n",
      "Iteration 169, loss = 0.37741226\n",
      "Iteration 170, loss = 0.37716447\n",
      "Iteration 171, loss = 0.37686985\n",
      "Iteration 172, loss = 0.37663556\n",
      "Iteration 173, loss = 0.37636698\n",
      "Iteration 174, loss = 0.37611520\n",
      "Iteration 175, loss = 0.37585893\n",
      "Iteration 176, loss = 0.37563267\n",
      "Iteration 177, loss = 0.37536569\n",
      "Iteration 178, loss = 0.37510122\n",
      "Iteration 179, loss = 0.37490307\n",
      "Iteration 180, loss = 0.37461071\n",
      "Iteration 181, loss = 0.37441608\n",
      "Iteration 182, loss = 0.37411795\n",
      "Iteration 183, loss = 0.37390221\n",
      "Iteration 184, loss = 0.37365327\n",
      "Iteration 185, loss = 0.37346406\n",
      "Iteration 186, loss = 0.37321908\n",
      "Iteration 187, loss = 0.37295134\n",
      "Iteration 188, loss = 0.37273605\n",
      "Iteration 189, loss = 0.37251403\n",
      "Iteration 190, loss = 0.37225223\n",
      "Iteration 191, loss = 0.37202388\n",
      "Iteration 192, loss = 0.37176980\n",
      "Iteration 193, loss = 0.37153127\n",
      "Iteration 194, loss = 0.37132928\n",
      "Iteration 195, loss = 0.37111639\n",
      "Iteration 196, loss = 0.37086931\n",
      "Iteration 197, loss = 0.37064151\n",
      "Iteration 198, loss = 0.37038284\n",
      "Iteration 199, loss = 0.37019998\n",
      "Iteration 200, loss = 0.36994550\n",
      "Iteration 201, loss = 0.36976232\n",
      "Iteration 202, loss = 0.36949250\n",
      "Iteration 203, loss = 0.36930117\n",
      "Iteration 204, loss = 0.36902079\n",
      "Iteration 205, loss = 0.36881732\n",
      "Iteration 206, loss = 0.36863222\n",
      "Iteration 207, loss = 0.36842239\n",
      "Iteration 208, loss = 0.36820540\n",
      "Iteration 209, loss = 0.36796131\n",
      "Iteration 210, loss = 0.36780727\n",
      "Iteration 211, loss = 0.36750122\n",
      "Iteration 212, loss = 0.36732170\n",
      "Iteration 213, loss = 0.36712108\n",
      "Iteration 214, loss = 0.36685513\n",
      "Iteration 215, loss = 0.36665556\n",
      "Iteration 216, loss = 0.36641328\n",
      "Iteration 217, loss = 0.36621757\n",
      "Iteration 218, loss = 0.36596626\n",
      "Iteration 219, loss = 0.36576900\n",
      "Iteration 220, loss = 0.36555119\n",
      "Iteration 221, loss = 0.36536439\n",
      "Iteration 222, loss = 0.36513657\n",
      "Iteration 223, loss = 0.36494436\n",
      "Iteration 224, loss = 0.36471511\n",
      "Iteration 225, loss = 0.36452824\n",
      "Iteration 226, loss = 0.36435082\n",
      "Iteration 227, loss = 0.36411568\n",
      "Iteration 228, loss = 0.36400341\n",
      "Iteration 229, loss = 0.36370853\n",
      "Iteration 230, loss = 0.36348059\n",
      "Iteration 231, loss = 0.36333775\n",
      "Iteration 232, loss = 0.36311684\n",
      "Iteration 233, loss = 0.36292378\n",
      "Iteration 234, loss = 0.36272359\n",
      "Iteration 235, loss = 0.36251339\n",
      "Iteration 236, loss = 0.36232172\n",
      "Iteration 237, loss = 0.36211729\n",
      "Iteration 238, loss = 0.36195518\n",
      "Iteration 239, loss = 0.36173101\n",
      "Iteration 240, loss = 0.36154864\n",
      "Iteration 241, loss = 0.36140329\n",
      "Iteration 242, loss = 0.36116708\n",
      "Iteration 243, loss = 0.36098492\n",
      "Iteration 244, loss = 0.36080874\n",
      "Iteration 245, loss = 0.36064135\n",
      "Iteration 246, loss = 0.36045211\n",
      "Iteration 247, loss = 0.36024357\n",
      "Iteration 248, loss = 0.36010581\n",
      "Iteration 249, loss = 0.35985965\n",
      "Iteration 250, loss = 0.35971433\n",
      "Iteration 251, loss = 0.35951929\n",
      "Iteration 252, loss = 0.35933147\n",
      "Iteration 253, loss = 0.35920988\n",
      "Iteration 254, loss = 0.35900282\n",
      "Iteration 255, loss = 0.35878601\n",
      "Iteration 256, loss = 0.35863576\n",
      "Iteration 257, loss = 0.35846853\n",
      "Iteration 258, loss = 0.35830808\n",
      "Iteration 259, loss = 0.35809464\n",
      "Iteration 260, loss = 0.35792167\n",
      "Iteration 261, loss = 0.35774485\n",
      "Iteration 262, loss = 0.35756329\n",
      "Iteration 263, loss = 0.35742762\n",
      "Iteration 264, loss = 0.35721492\n",
      "Iteration 265, loss = 0.35710625\n",
      "Iteration 266, loss = 0.35689322\n",
      "Iteration 267, loss = 0.35674372\n",
      "Iteration 268, loss = 0.35658477\n",
      "Iteration 269, loss = 0.35638766\n",
      "Iteration 270, loss = 0.35624837\n",
      "Iteration 271, loss = 0.35606444\n",
      "Iteration 272, loss = 0.35592962\n",
      "Iteration 273, loss = 0.35576436\n",
      "Iteration 274, loss = 0.35557130\n",
      "Iteration 275, loss = 0.35541895\n",
      "Iteration 276, loss = 0.35533627\n",
      "Iteration 277, loss = 0.35509701\n",
      "Iteration 278, loss = 0.35495892\n",
      "Iteration 279, loss = 0.35482502\n",
      "Iteration 280, loss = 0.35464646\n",
      "Iteration 281, loss = 0.35455600\n",
      "Iteration 282, loss = 0.35431964\n",
      "Iteration 283, loss = 0.35415308\n",
      "Iteration 284, loss = 0.35403538\n",
      "Iteration 285, loss = 0.35389126\n",
      "Iteration 286, loss = 0.35371589\n",
      "Iteration 287, loss = 0.35356008\n",
      "Iteration 288, loss = 0.35340286\n",
      "Iteration 289, loss = 0.35325345\n",
      "Iteration 290, loss = 0.35309045\n",
      "Iteration 291, loss = 0.35297007\n",
      "Iteration 292, loss = 0.35277897\n",
      "Iteration 293, loss = 0.35268131\n",
      "Iteration 294, loss = 0.35250135\n",
      "Iteration 295, loss = 0.35235781\n",
      "Iteration 296, loss = 0.35225235\n",
      "Iteration 297, loss = 0.35208854\n",
      "Iteration 298, loss = 0.35193169\n",
      "Iteration 299, loss = 0.35175991\n",
      "Iteration 300, loss = 0.35163964\n",
      "Iteration 301, loss = 0.35153519\n",
      "Iteration 302, loss = 0.35139296\n",
      "Iteration 303, loss = 0.35117246\n",
      "Iteration 304, loss = 0.35108736\n",
      "Iteration 305, loss = 0.35087587\n",
      "Iteration 306, loss = 0.35076075\n",
      "Iteration 307, loss = 0.35065325\n",
      "Iteration 308, loss = 0.35047052\n",
      "Iteration 309, loss = 0.35037063\n",
      "Iteration 310, loss = 0.35027587\n",
      "Iteration 311, loss = 0.35007962\n",
      "Iteration 312, loss = 0.34991413\n",
      "Iteration 313, loss = 0.34977090\n",
      "Iteration 314, loss = 0.34963625\n",
      "Iteration 315, loss = 0.34956028\n",
      "Iteration 316, loss = 0.34938364\n",
      "Iteration 317, loss = 0.34925411\n",
      "Iteration 318, loss = 0.34910832\n",
      "Iteration 319, loss = 0.34896427\n",
      "Iteration 320, loss = 0.34882186\n",
      "Iteration 321, loss = 0.34874058\n",
      "Iteration 322, loss = 0.34853341\n",
      "Iteration 323, loss = 0.34841852\n",
      "Iteration 324, loss = 0.34827938\n",
      "Iteration 325, loss = 0.34818319\n",
      "Iteration 326, loss = 0.34804708\n",
      "Iteration 327, loss = 0.34789942\n",
      "Iteration 328, loss = 0.34775157\n",
      "Iteration 329, loss = 0.34763654\n",
      "Iteration 330, loss = 0.34764485\n",
      "Iteration 331, loss = 0.34732775\n",
      "Iteration 332, loss = 0.34721780\n",
      "Iteration 333, loss = 0.34712920\n",
      "Iteration 334, loss = 0.34702642\n",
      "Iteration 335, loss = 0.34682284\n",
      "Iteration 336, loss = 0.34674030\n",
      "Iteration 337, loss = 0.34656146\n",
      "Iteration 338, loss = 0.34650137\n",
      "Iteration 339, loss = 0.34634758\n",
      "Iteration 340, loss = 0.34618642\n",
      "Iteration 341, loss = 0.34608505\n",
      "Iteration 342, loss = 0.34597483\n",
      "Iteration 343, loss = 0.34584859\n",
      "Iteration 344, loss = 0.34573652\n",
      "Iteration 345, loss = 0.34562541\n",
      "Iteration 346, loss = 0.34548876\n",
      "Iteration 347, loss = 0.34532624\n",
      "Iteration 348, loss = 0.34520644\n",
      "Iteration 349, loss = 0.34509599\n",
      "Iteration 350, loss = 0.34499088\n",
      "Iteration 351, loss = 0.34490177\n",
      "Iteration 352, loss = 0.34474570\n",
      "Iteration 353, loss = 0.34462837\n",
      "Iteration 354, loss = 0.34447346\n",
      "Iteration 355, loss = 0.34440936\n",
      "Iteration 356, loss = 0.34426650\n",
      "Iteration 357, loss = 0.34413217\n",
      "Iteration 358, loss = 0.34398591\n",
      "Iteration 359, loss = 0.34387722\n",
      "Iteration 360, loss = 0.34383820\n",
      "Iteration 361, loss = 0.34364610\n",
      "Iteration 362, loss = 0.34351613\n",
      "Iteration 363, loss = 0.34360643\n",
      "Iteration 364, loss = 0.34330489\n",
      "Iteration 365, loss = 0.34320200\n",
      "Iteration 366, loss = 0.34303725\n",
      "Iteration 367, loss = 0.34298564\n",
      "Iteration 368, loss = 0.34285477\n",
      "Iteration 369, loss = 0.34274226\n",
      "Iteration 370, loss = 0.34264402\n",
      "Iteration 371, loss = 0.34251543\n",
      "Iteration 372, loss = 0.34245080\n",
      "Iteration 373, loss = 0.34231466\n",
      "Iteration 374, loss = 0.34215423\n",
      "Iteration 375, loss = 0.34203441\n",
      "Iteration 376, loss = 0.34199061\n",
      "Iteration 377, loss = 0.34189966\n",
      "Iteration 378, loss = 0.34171413\n",
      "Iteration 379, loss = 0.34164476\n",
      "Iteration 380, loss = 0.34156416\n",
      "Iteration 381, loss = 0.34144972\n",
      "Iteration 382, loss = 0.34133819\n",
      "Iteration 383, loss = 0.34126526\n",
      "Iteration 384, loss = 0.34109453\n",
      "Iteration 385, loss = 0.34101464\n",
      "Iteration 386, loss = 0.34091954\n",
      "Iteration 387, loss = 0.34077136\n",
      "Iteration 388, loss = 0.34067853\n",
      "Iteration 389, loss = 0.34059760\n",
      "Iteration 390, loss = 0.34047877\n",
      "Iteration 391, loss = 0.34038294\n",
      "Iteration 392, loss = 0.34031663\n",
      "Iteration 393, loss = 0.34016556\n",
      "Iteration 394, loss = 0.34009132\n",
      "Iteration 395, loss = 0.34001805\n",
      "Iteration 396, loss = 0.33989796\n",
      "Iteration 397, loss = 0.33976402\n",
      "Iteration 398, loss = 0.33965344\n",
      "Iteration 399, loss = 0.33955876\n",
      "Iteration 400, loss = 0.33946296\n",
      "Iteration 401, loss = 0.33935083\n",
      "Iteration 402, loss = 0.33923854\n",
      "Iteration 403, loss = 0.33915339\n",
      "Iteration 404, loss = 0.33904819\n",
      "Iteration 405, loss = 0.33891354\n",
      "Iteration 406, loss = 0.33883497\n",
      "Iteration 407, loss = 0.33873760\n",
      "Iteration 408, loss = 0.33868408\n",
      "Iteration 409, loss = 0.33855769\n",
      "Iteration 410, loss = 0.33845348\n",
      "Iteration 411, loss = 0.33829258\n",
      "Iteration 412, loss = 0.33824768\n",
      "Iteration 413, loss = 0.33817323\n",
      "Iteration 414, loss = 0.33806867\n",
      "Iteration 415, loss = 0.33796256\n",
      "Iteration 416, loss = 0.33785420\n",
      "Iteration 417, loss = 0.33782392\n",
      "Iteration 418, loss = 0.33766392\n",
      "Iteration 419, loss = 0.33752667\n",
      "Iteration 420, loss = 0.33745490\n",
      "Iteration 421, loss = 0.33735848\n",
      "Iteration 422, loss = 0.33725369\n",
      "Iteration 423, loss = 0.33717482\n",
      "Iteration 424, loss = 0.33705735\n",
      "Iteration 425, loss = 0.33701311\n",
      "Iteration 426, loss = 0.33687534\n",
      "Iteration 427, loss = 0.33677952\n",
      "Iteration 428, loss = 0.33665588\n",
      "Iteration 429, loss = 0.33663540\n",
      "Iteration 430, loss = 0.33655018\n",
      "Iteration 431, loss = 0.33640741\n",
      "Iteration 432, loss = 0.33634786\n",
      "Iteration 433, loss = 0.33622273\n",
      "Iteration 434, loss = 0.33610961\n",
      "Iteration 435, loss = 0.33601814\n",
      "Iteration 436, loss = 0.33590168\n",
      "Iteration 437, loss = 0.33580222\n",
      "Iteration 438, loss = 0.33573746\n",
      "Iteration 439, loss = 0.33568161\n",
      "Iteration 440, loss = 0.33554005\n",
      "Iteration 441, loss = 0.33548226\n",
      "Iteration 442, loss = 0.33541707\n",
      "Iteration 443, loss = 0.33526603\n",
      "Iteration 444, loss = 0.33516753\n",
      "Iteration 445, loss = 0.33509140\n",
      "Iteration 446, loss = 0.33505603\n",
      "Iteration 447, loss = 0.33491963\n",
      "Iteration 448, loss = 0.33481451\n",
      "Iteration 449, loss = 0.33488223\n",
      "Iteration 450, loss = 0.33466254\n",
      "Iteration 451, loss = 0.33454614\n",
      "Iteration 452, loss = 0.33444209\n",
      "Iteration 453, loss = 0.33434443\n",
      "Iteration 454, loss = 0.33428673\n",
      "Iteration 455, loss = 0.33418265\n",
      "Iteration 456, loss = 0.33407626\n",
      "Iteration 457, loss = 0.33395486\n",
      "Iteration 458, loss = 0.33391091\n",
      "Iteration 459, loss = 0.33379201\n",
      "Iteration 460, loss = 0.33371411\n",
      "Iteration 461, loss = 0.33362863\n",
      "Iteration 462, loss = 0.33354802\n",
      "Iteration 463, loss = 0.33341382\n",
      "Iteration 464, loss = 0.33333485\n",
      "Iteration 465, loss = 0.33324175\n",
      "Iteration 466, loss = 0.33328227\n",
      "Iteration 467, loss = 0.33303291\n",
      "Iteration 468, loss = 0.33296182\n",
      "Iteration 469, loss = 0.33292886\n",
      "Iteration 470, loss = 0.33283271\n",
      "Iteration 471, loss = 0.33276595\n",
      "Iteration 472, loss = 0.33261399\n",
      "Iteration 473, loss = 0.33265557\n",
      "Iteration 474, loss = 0.33243850\n",
      "Iteration 475, loss = 0.33241279\n",
      "Iteration 476, loss = 0.33229808\n",
      "Iteration 477, loss = 0.33226096\n",
      "Iteration 478, loss = 0.33215819\n",
      "Iteration 479, loss = 0.33199430\n",
      "Iteration 480, loss = 0.33193005\n",
      "Iteration 481, loss = 0.33180968\n",
      "Iteration 482, loss = 0.33181872\n",
      "Iteration 483, loss = 0.33168141\n",
      "Iteration 484, loss = 0.33155781\n",
      "Iteration 485, loss = 0.33150184\n",
      "Iteration 486, loss = 0.33148154\n",
      "Iteration 487, loss = 0.33126441\n",
      "Iteration 488, loss = 0.33120104\n",
      "Iteration 489, loss = 0.33112003\n",
      "Iteration 490, loss = 0.33100150\n",
      "Iteration 491, loss = 0.33093340\n",
      "Iteration 492, loss = 0.33082256\n",
      "Iteration 493, loss = 0.33076608\n",
      "Iteration 494, loss = 0.33069877\n",
      "Iteration 495, loss = 0.33064430\n",
      "Iteration 496, loss = 0.33062411\n",
      "Iteration 497, loss = 0.33043042\n",
      "Iteration 498, loss = 0.33036004\n",
      "Iteration 499, loss = 0.33028255\n",
      "Iteration 500, loss = 0.33024293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajink\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8622710622710623"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train our neural network model on balanced data\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=500, alpha=0.0001,\n",
    "                     solver='sgd', verbose=10,  random_state=21,tol=0.000000001)\n",
    "mlp_results = mlp.fit(train_features, train_labels)\n",
    "pred = mlp.predict(train_features)\n",
    "accuracy_score(train_labels, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not sure this is the best implementation of neural network model. I just tried to build a very basic model and I can already see it's not too efficient. But, the good thing is we now have a more accurate model, which means that neural network are a great tool to solve our problem. I plan to make a separate kernel to build a neural network model and fine-tune it to make it more efficient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
